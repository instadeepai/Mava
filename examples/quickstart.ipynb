{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uCEQLS3zZUn"
      },
      "source": [
        "# JAX Mava Quickstart Notebook\n",
        "<img src=\"https://raw.githubusercontent.com/instadeepai/Mava/develop/docs/images/mava.png\" />\n",
        "\n",
        "### This notebook provides an easy introducion to the [Mava](https://github.com/instadeepai/Mava) framework by showing how to construct a multi-agent system, and train it from scratch in a simple evironment. \n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/instadeepai/Mava/blob/develop/examples/quickstart.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEAq7x7ff1fE"
      },
      "source": [
        "### 1. Installing packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gXH-DtX6OtI"
      },
      "source": [
        "We start by installing the necessary packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pl4ed6X22tZq"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "#@title Install required packages. (Run Cell)\n",
        "# Fix common bug in installing 'box-2d'\n",
        "!apt-get install swig\n",
        "\n",
        "# Get Mava\n",
        "%pip install \"id-mava[reverb,jax,envs] @ git+https://github.com/instadeepai/mava.git\"\n",
        "\n",
        "# Visualisation stuff\n",
        "!apt-get update -y &&  apt-get install -y xvfb &&  apt-get install -y python-opengl && apt-get install ffmpeg && pip install pyvirtualdisplay \n",
        "\n",
        "# Colab has an old version of cloudpickle\n",
        "%pip install cloudpickle -U\n",
        "\n",
        "# Agent visualisation dependencies\n",
        "!apt-get install -y python-opengl\n",
        "!apt-get install -y xvfb python-opengl ffmpeg\n",
        "%pip install pyglet==1.3.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfyWPW0LHqaB"
      },
      "source": [
        "At this point, it is necessary to restart the runtime. On the top menu, click `Runtime -> Restart runtime`. Alternatively, use the shortcut, `âŒ˜/Ctrl + M .`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SGFGmWnhuI2"
      },
      "source": [
        "### 2. Import the necessary modules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjPRSgi56Mi7"
      },
      "source": [
        "We first import the necessary modules that will be used to construct the multi-agent system and visualise the agents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SvWrsWExz31"
      },
      "outputs": [],
      "source": [
        "#@title Import required packages. (Run Cell)\n",
        "import functools\n",
        "from datetime import datetime\n",
        "from typing import Any\n",
        "\n",
        "import optax\n",
        "from absl import app, flags\n",
        "\n",
        "from mava.systems import ippo\n",
        "from mava.utils.environments import debugging_utils\n",
        "from mava.utils.loggers import logger_utils\n",
        "from mava.components.building.environments import MonitorExecutorEnvironmentLoop\n",
        "\n",
        "# Imports for agent visualisation\n",
        "import os\n",
        "from IPython.display import HTML\n",
        "from pyvirtualdisplay import Display\n",
        "import pyglet\n",
        "\n",
        "display = Display(visible=0, size=(1024, 768))\n",
        "display.start()\n",
        "config = pyglet.gl.Config(double_buffer=True)\n",
        "os.environ[\"DISPLAY\"] = \":\" + str(display.display)\n",
        "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"]= \"false\"\n",
        "os.environ['PYGLET_SHADOW_WINDOW'] = '0'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ul_phKL7h4Vq"
      },
      "source": [
        "### 3. Train an `IPPO` system"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8XqA9M2iyK_"
      },
      "source": [
        "For this example, we will train IPPO on the simple-spread environment. This environment has 3 agents that all need to move to specific locations without bumping into each other. We start by defining the network architecture and network optimisers for our agents. The default network file for ippo can be found [here](https://github.com/instadeepai/Mava/blob/develop/mava/systems/jax/ippo/networks.py)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJ4-cN2dkXjq"
      },
      "outputs": [],
      "source": [
        "# Create the network factory.\n",
        "def network_factory(*args: Any, **kwargs: Any) -> Any:\n",
        "    return ippo.make_default_networks(  # type: ignore\n",
        "        policy_layer_sizes=(64, 64),\n",
        "        critic_layer_sizes=(64, 64, 64),\n",
        "        *args,\n",
        "        **kwargs,\n",
        "    )\n",
        "  \n",
        "# Optimisers.\n",
        "policy_optimiser = optax.chain(\n",
        "    optax.clip_by_global_norm(40.0), optax.scale_by_adam(), optax.scale(-1e-4)\n",
        ")\n",
        "\n",
        "critic_optimiser = optax.chain(\n",
        "    optax.clip_by_global_norm(40.0), optax.scale_by_adam(), optax.scale(-1e-4)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohA5m0REjhu-"
      },
      "source": [
        "We now select the environment we want to train on. We will use the [simple spread](https://github.com/instadeepai/Mava#debugging) environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fw_4dR1jj-Wv"
      },
      "outputs": [],
      "source": [
        "env_name = \"simple_spread\"\n",
        "action_space = \"discrete\"\n",
        "\n",
        "environment_factory = functools.partial(\n",
        "    debugging_utils.make_environment,\n",
        "    env_name=env_name,\n",
        "    action_space=action_space,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avvSeVahk_Nt"
      },
      "source": [
        "Next, we specify the logging and checkpointing configuration for our system. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8J05yDlk-ya"
      },
      "outputs": [],
      "source": [
        "# Directory to store checkpoints and log data. \n",
        "base_dir = \"~/mava\"\n",
        "\n",
        "# File name \n",
        "mava_id = datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
        "\n",
        "# Log every [log_every] seconds\n",
        "log_every = 15\n",
        "logger_factory = functools.partial(\n",
        "    logger_utils.make_logger,\n",
        "    directory=base_dir,\n",
        "    to_terminal=True,\n",
        "    to_tensorboard=True,\n",
        "    time_stamp=mava_id,\n",
        "    time_delta=log_every,\n",
        ")\n",
        "\n",
        "# Checkpointer appends \"Checkpoints\" to experiment_path\n",
        "experiment_path = f\"{base_dir}/{mava_id}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5i3tj4h-lTm4"
      },
      "source": [
        "Finally, we construct our multi-agent PPO system. A system in Mava represents the complete multi-agent setup and when it is launched, it runs executors (experience generators), a trainer (the network updater), a data server (buffer for experience) and a parameter server (to store network parameters)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CS618jAtxM1h",
        "outputId": "893cab95-d27a-470f-9a0d-0b4f2d459807"
      },
      "outputs": [],
      "source": [
        "# Create the system.\n",
        "system = ippo.IPPOSystem()\n",
        "\n",
        "# Add the gameplay monitor component\n",
        "system.update(MonitorExecutorEnvironmentLoop)\n",
        "\n",
        "# Build the system.\n",
        "system.build(\n",
        "    environment_factory=environment_factory,\n",
        "    network_factory=network_factory,\n",
        "    logger_factory=logger_factory,\n",
        "    experiment_path=experiment_path,\n",
        "    policy_optimiser=policy_optimiser,\n",
        "    critic_optimiser=critic_optimiser,\n",
        "    run_evaluator=True,\n",
        "    epoch_batch_size=5,\n",
        "    num_epochs=15,\n",
        "    num_executors=2,\n",
        "    multi_process=True,\n",
        "    record_every=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fH3u39Cfa-bQ"
      },
      "source": [
        "In this example, we use IPPO which is already implemented inside Mava. However, we add the `MonitorExecutorEnvironmentLoop` component to record episodes for later inspection.  We use `system.update` because we already have an environment loop component inside Mava and we are simply updating it. If we want to add an entirely new component to the system, we use `system.add`. \n",
        "\n",
        "Mava is designed with flexibility in mind and allows users to easily build on top of existing systems. A system is just a collection of components, which can be though of as self-contained pieces of code (building block) that add functionality to a system. The IPPO system's components can be found [here](https://github.com/instadeepai/Mava/blob/develop/mava/systems/jax/ippo/system.py). \n",
        "\n",
        "The arguments we provide to our system build overwrite the default config values from the existing components in the system:\n",
        "\n",
        "`environment_factory` - used to construct the environment.\n",
        "\n",
        "`network_factory` - used to construct the agent networks.\n",
        "\n",
        "`logger_factory` - used to construct the loggers.\n",
        "\n",
        "`experiment_path` - the destination to save the experiment results.\n",
        "\n",
        "`policy_optimiser` - the optimiser used by the trainer to updated the policy weights.\n",
        "\n",
        "`critic_optimiser` - the optimiser used by the trainer to updated the critic weights.\n",
        "\n",
        "`run_evaluator` - a flag indicating whether a separate environment process should be run that tracks the system's performance using Tensorboard and possibly gameplay recordings.\n",
        "\n",
        "`epoch_batch_size` - the batch size to use in the trainer when updating the agent networks.\n",
        "\n",
        "`num_epochs` - the number of epochs to train on sampled data before discarding it.\n",
        "\n",
        "`num_executors` - the number of experience generators (workers) to run in parallel.\n",
        "\n",
        "`multi_process` - determines whether the code is run using multiple processes or using a single process. We are using the multiple processor setup for faster training.\n",
        "\n",
        "`record_every` - determines how often the evaluator should record a gameplay video. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBWiibHIleQk"
      },
      "source": [
        "We now run the system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gsoLWPTClnMt"
      },
      "outputs": [],
      "source": [
        "# Launch the system.\n",
        "system.launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHygoBPW-3KV"
      },
      "source": [
        "### 4. Visuallise our training results using Tensorboard\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAGEzi_rWhOh"
      },
      "source": [
        "Load the tensorboard exension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l181SBwtBo9M"
      },
      "outputs": [],
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJl7LKmHAOk-"
      },
      "source": [
        "To view training results, start tensorboard and point it to the Mava `experiment_path` (where logs are saved). You might have to wait a few seconds and then refresh Tensorboard to see the logs.\n",
        "\n",
        "A good score is a `evaluator/RawEpisodeReturn` between 30-40 after about 5 minutes of training (remember to refresh Tensorboard). Although this system is stochastic, it should reach that score after a few minutes.    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fU3yEhdFx1O"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir ~/mava/$mava_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDlUXGltyVhM"
      },
      "source": [
        "### 5. View agent recordings\n",
        "Once a good score is reached, you can view the learned multi-agent behaviour by viewing the agent recordings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2l8o2zDBbuN"
      },
      "source": [
        "Check if any agent recordings are available. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXB1IKfysMT6",
        "outputId": "29e677af-95d7-466e-a52b-27bfdb94043f"
      },
      "outputs": [],
      "source": [
        "! ls ~/mava/$mava_id/recordings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjcnXbl7BfJc"
      },
      "source": [
        "View the latest agent recording. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEEshoXd2K1S"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import os \n",
        "import IPython\n",
        "\n",
        "# Recordings\n",
        "list_of_files = glob.glob(f\"/root/mava/{mava_id}/recordings/*.html\")\n",
        "\n",
        "if(len(list_of_files) == 0):\n",
        "  print(\"No recordings are available yet. Please wait or run the 'Run Multi-Agent PPO System.' cell if you haven't already done this.\")\n",
        "else:\n",
        "  latest_file = max(list_of_files, key=os.path.getctime)\n",
        "  print(\"Run the next cell to visualize your agents!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJ33l0uIJ9xB"
      },
      "source": [
        "If the agents are trained (*usually in about 5 minutes...*), they should move to the assigned landmarks.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/instadeepai/Mava/develop/docs/images/simple_spread.png\" width=\"250\" height=\"250\" />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95GOv5vc5z5Q"
      },
      "outputs": [],
      "source": [
        "# Latest file needs to point to the latest recording\n",
        "IPython.display.HTML(filename=latest_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryQ9EhumnGBS"
      },
      "source": [
        "That's it! You have successfully trained a multi-agent system in Mava."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYekMtHB26yL"
      },
      "source": [
        "## What's next?\n",
        "\n",
        "Now that you have an appreciation for the flexibility in Mava's design, let us now build our own custom component and add that to the existing IPPO system. Each component places self-contained pieces of code at specific points when we run the system, referred to as \"hooks\": and each of these hook function names starts with the \"on\" keyword. For example, When the system is building, it calls [these](https://github.com/instadeepai/Mava/blob/develop/mava/systems/jax/builder.py) hooks defined in the builder, and when it is executing (generating training experience), it can call the hooks defined in the [executor](https://github.com/instadeepai/Mava/blob/develop/mava/systems/jax/executor.py), [trainer](https://github.com/instadeepai/Mava/blob/develop/mava/systems/jax/trainer.py) or [parameter_server](https://github.com/instadeepai/Mava/blob/develop/mava/systems/jax/parameter_server.py).\n",
        "\n",
        "A component must \"subscribe\" to the relevant hooks, depending on when the code needs to be executed. For example, after a training step (on_training_step_end()) or when selecting actions for each agent in order to generate training experience (on_execution_select_actions())\n",
        "\n",
        "To illustrate this point, we will update the [advantage estimation component](https://github.com/instadeepai/Mava/blob/develop/mava/components/jax/training/advantage_estimation.py) with a simpler advantage estimate and override the on_training_utility_fns() hook defined in the trainer. This component defines a function called `gae_fn`, which is called by the system to calculate an advantage estimate when training. Notice below that we store this new advantage function inside `trainer.store.gae_fn`. The store is a place where components can save variables for other components to access."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94w0YDkodOce"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "#@title Kill old runs. (Run Cell)\n",
        "!ps aux  |  grep -i launchpad  |  awk '{print $2}'  |  xargs sudo kill -9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKPKCqB21XMS"
      },
      "source": [
        "We start by defining our new component below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0HOxOnVmq93"
      },
      "outputs": [],
      "source": [
        "\"\"\"Trainer components for advantage calculations.\"\"\"\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Optional, Tuple, Type\n",
        "from mava.components.training.advantage_estimation import GAE\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import rlax\n",
        "\n",
        "from mava.callbacks import Callback\n",
        "from mava.components.training.base import Utility\n",
        "from mava.core_jax import SystemTrainer\n",
        "\n",
        "@dataclass\n",
        "class AConfig:\n",
        "    a_lambda: float = 0.95\n",
        "\n",
        "\n",
        "class simpler_advantage_estimate(GAE):\n",
        "    def __init__(\n",
        "        self,\n",
        "        config: AConfig = AConfig(),\n",
        "    ):\n",
        "        self.config = config\n",
        "\n",
        "    def on_training_utility_fns(self, trainer: SystemTrainer) -> None:\n",
        "        def simpler_advantage_estimate(\n",
        "            rewards: jnp.ndarray, discounts: jnp.ndarray, values: jnp.ndarray\n",
        "        ) -> Tuple[jnp.ndarray, jnp.ndarray]:\n",
        "\n",
        "\n",
        "            # Instead of using the GAE we use a simpler one step\n",
        "            # advantage estimate.\n",
        "\n",
        "            # Pad the rewards so that rewards at the end can also be calculated.\n",
        "            zeros_mask = jnp.zeros(shape=rewards.shape)\n",
        "            padded_rewards = jnp.concatenate([rewards, zeros_mask], axis=0)\n",
        "            cum_rewards = rewards.copy()\n",
        "            seq_len = len(rewards)\n",
        "            for i in range(1, seq_len):\n",
        "                cum_rewards+=padded_rewards[i : i +seq_len]\\\n",
        "                              *jnp.power(self.config.a_lambda, i)\n",
        "\n",
        "            # Calculate the advantage estimate.\n",
        "            advantages = cum_rewards[:-1] - values[:-1]\n",
        "            \n",
        "            # Stop gradients from flowing through the advantage estimate.\n",
        "            advantages = jax.lax.stop_gradient(advantages)\n",
        "\n",
        "            # Set the target values and stop gradients from flowing backwards\n",
        "            # through the target values.\n",
        "            target_values = cum_rewards[:-1]\n",
        "            target_values = jax.lax.stop_gradient(target_values)\n",
        "\n",
        "            return advantages, target_values\n",
        "\n",
        "        trainer.store.gae_fn = simpler_advantage_estimate\n",
        "\n",
        "    @staticmethod\n",
        "    def name() -> str:\n",
        "        return \"gae_fn\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcYvshTpmu_J"
      },
      "source": [
        "Now that we have this new component we can add it to the PPO system using `system.update`. You can retrain the system by executing the cell below and again run steps 4 and 5 (above) to view the training results and gameplay footage. Does this system perform better or worse than the previous system? \n",
        "\n",
        "Feel free to update the advantage component to see if you can achieve better training results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9efhqaiq-QL"
      },
      "outputs": [],
      "source": [
        "# File name \n",
        "mava_id = datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
        "\n",
        "# Log every [log_every] seconds\n",
        "log_every = 15\n",
        "logger_factory = functools.partial(\n",
        "    logger_utils.make_logger,\n",
        "    directory=base_dir,\n",
        "    to_terminal=True,\n",
        "    to_tensorboard=True,\n",
        "    time_stamp=mava_id,\n",
        "    time_delta=log_every,\n",
        ")\n",
        "\n",
        "# Checkpointer appends \"Checkpoints\" to experiment_path\n",
        "experiment_path = f\"{base_dir}/{mava_id}\"\n",
        "\n",
        "# Create the system.\n",
        "system = ippo.IPPOSystem()\n",
        "\n",
        "# Add the gameplay monitor component\n",
        "system.update(MonitorExecutorEnvironmentLoop)\n",
        "\n",
        "# Update the system with out custom component.\n",
        "system.update(simpler_advantage_estimate)\n",
        "\n",
        "# Build the system.\n",
        "system.build(\n",
        "    environment_factory=environment_factory,\n",
        "    network_factory=network_factory,\n",
        "    logger_factory=logger_factory,\n",
        "    experiment_path=experiment_path,\n",
        "    policy_optimiser=policy_optimiser,\n",
        "    critic_optimiser=critic_optimiser,\n",
        "    run_evaluator=True,\n",
        "    epoch_batch_size=5,\n",
        "    num_epochs=15,\n",
        "    num_executors=1,\n",
        "    multi_process=True,\n",
        "    record_every=10,\n",
        " )\n",
        "\n",
        "# Launch the system.\n",
        "system.launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlzWiU7SVhpw"
      },
      "source": [
        "Congratulations! You have created your own custom system. We hope that this tutorial has given you a taste of what Mava is capable of. We are excited to see how you use the repo. For more examples using different systems, environments and architectures, visit our [github page](https://github.com/instadeepai/Mava/tree/develop/ examples)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
