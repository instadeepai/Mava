# --- Defaults ---

# --- Experiment params ---
defaults:
  - _self_
  - rware_scenario: tiny-2ag

# --- Environment ---
env_name: RobotWarehouse-v0

num_envs: 16  # Number of vectorised environments per device.
num_updates: 1000 # Number of updates
seed: 42

# --- Agent observations ---
add_agent_id: True

# --- RL hyperparameters ---
actor_lr: 2.5e-4 # Learning rate for actor network
critic_lr: 2.5e-4 # Learning rate for critic network
update_batch_size: 2 # Number of vectorised gradient updates per device.
rollout_length: 128 # Number of environment steps per vectorised environment.

ppo_epochs: 4 # Number of ppo epochs per training data batch.
num_minibatches: 2 # Number of minibatches per ppo epoch.
gamma: 0.99 # Discounting factor.
gae_lambda: 0.95 # Lambda value for GAE computation.
clip_eps: 0.2 # Clipping value for PPO updates and value function.
ent_coef: 0.01 # Entropy regularisation term for loss function.
vf_coef: 0.5 # Critic weight in
max_grad_norm: 0.5 # Maximum norm of the gradients for a weight update.

# --- Evaluation ---
num_eval_episodes: 32 # Episodes per evaluation
num_evaluation: 200 # Number of evenly spaced evaluations to perform during training.
evaluation_greedy: False # Evaluate the policy greedily. If True the policy will select
  # an action which corresponds to the greatest logit. If false, the policy will sample
  # from the logits.
absolute_metric: True # Whether the absolute metric should be computed. For more details
  # on the absolute metric please see: https://arxiv.org/abs/2209.10485

# --- Logging options ---
use_tf: False
use_sacred: True
base_exp_path: results
system_name: ~  # this is manually set inside each file depending on which system is run
# Neptune logging
use_neptune: False
neptune_project: Instadeep/Mava
neptune_tag: [rware]
name: None # Unique ID from logging.
